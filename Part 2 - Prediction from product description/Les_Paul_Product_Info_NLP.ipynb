{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf145dd0-a907-42d7-866b-40e39dcfc8b0",
   "metadata": {},
   "source": [
    "# **Part 2 - Price prediction using Natural Language Processing (NLP)**\n",
    "\n",
    "In Part 1, I used attributes of Les Paul models to predict the retail price. This approach worked in principle but the model error was too large to be useful. A better approach would be to use the product description to identify key pieces of text which would allude to the price. For example, a basic guitar might have a very simplistic product description but an expensive guitar could have some unique keywords which would allude to a higher price tag. With this in mind, let's try NLP to predict guitar prices!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a991b93-1880-4875-bcd0-545b562905b0",
   "metadata": {},
   "source": [
    "## Extract, transform, load (ETL) pipeline\n",
    "Data engineering relies on (1) extraction of data from different sources, (2) transformation of the raw data into useful features, and (3) loading the data into a database. I made a `Scrapy` spider to extract guitar product descriptions, model names, and prices from the *Thomann* website (which I have already run and saved the data as a CSV file in the `guitar_scraper` directory). In step 2, we will format the price data from string to integer format and run some NLP techniques on the product descriptions, before carrying out step 3 where the transformed data will be saved in a local database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7500a37-91d7-4d4f-8f41-30b941f38086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4317 unique urls\n",
      "3973 unique descriptions\n",
      "344 duplicate descriptions\n",
      "\n",
      "Raw data:\n",
      "description    Custom Shop George Harrison \"Rocky\" Signature ...\n",
      "name                         Fender George Harrison \"Rocky\" MBPW\n",
      "price                                                    â‚¬26,590\n",
      "url            https://www.thomann.de/ie/fender_george_harris...\n",
      "Name: 445, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/martin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/martin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/martin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download([\"punkt\", \"wordnet\", \"stopwords\"])\n",
    "\n",
    "raw = pd.read_csv(\"../guitar_scraper/guitar_info.csv\")\n",
    "unique_urls = raw['url'].unique().shape[0]\n",
    "unique_desc = raw['description'].unique().shape[0]\n",
    "\n",
    "print(f\"{unique_urls} unique urls\")\n",
    "print(f\"{unique_desc} unique descriptions\")\n",
    "print(f\"{unique_urls - unique_desc} duplicate descriptions\")\n",
    "print(\"\\nRaw data:\")\n",
    "print(raw.iloc[445, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2244cc-50e7-4734-9f35-ef8e0f802b67",
   "metadata": {},
   "source": [
    "Product listings for a particular guitar model should be the same, so it is expected that there are some duplicate descriptions in the raw data. These guitars should only differ in terms of colour so we'll keep them in the dataset for now. To begin formatting our data, let's first convert the price column to integer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0881416-c62f-4684-8eff-56394b67a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the raw data to separate dataframe\n",
    "df = raw.copy()\n",
    "\n",
    "# transform the price column\n",
    "df[\"price\"] = df[\"price\"].apply(lambda x: int(x[1:].replace(\",\", \"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4ec46-c91e-479e-8b27-521caaf2d3bd",
   "metadata": {},
   "source": [
    "To process the text data, I'm borrowing a function I used for a different NLP project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eed244ce-1824-4c7a-af41-b42852089f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Process raw text into tokenized data for training (feature extraction)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "        tweet in string format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cleaned_tokens : list of tokenized strings\n",
    "\n",
    "    \"\"\"\n",
    "    # convert to lower case and only keep alphanumeric characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # split string into word tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove inflections of words with similar meaning\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            clean = lemmatizer.lemmatize(token).lower().strip()\n",
    "            cleaned_tokens.append(clean)\n",
    "    return cleaned_tokens\n",
    "\n",
    "tokens = tokenize(df[\"description\"][445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a69aec5-17cd-4be2-ac35-ac2fa9e250af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5083\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [2 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(df[\"description\"])\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "print(X.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
